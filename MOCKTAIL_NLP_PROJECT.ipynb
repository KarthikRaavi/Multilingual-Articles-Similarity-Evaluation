{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MOCKTAIL_NLP_PROJECT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Modules"
      ],
      "metadata": {
        "id": "KRkYkPmzU58-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "uS_ckoUlU5MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Data Extraction"
      ],
      "metadata": {
        "id": "rY9VLJm3QcB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('input.csv') # Here input.csv is the input csv file from the Semeval website\n",
        "a = df['pair_id'].tolist() # This array stores the pair_id for each sample\n",
        "b = [[i[8]+i[9],i[19]+i[20]] for i in a] \n",
        "s = \"/content/drive/MyDrive/training_data/output_dir/\" # This is the path to the data\n",
        "x = []    # This array stores the text data for all the News Article pair\n",
        "for i in range(len(a)):\n",
        "    o = []     # This array stores text data for a single News Article Pair\n",
        "    h = s+b[i][0]+'/'+a[i][:10]+'.json'    # Path corresponding to the 1st article\n",
        "    if os.path.exists(h)==False:     # If the path does not exist the text data will be [-1,-1]\n",
        "        x.append([-1,-1])\n",
        "        continue\n",
        "    with open(h,'r') as f:    # If the path exists we read the data\n",
        "        f = json.load(f)\n",
        "    o.append(f['text'])\n",
        "    h = s+b[i][1]+'/'+a[i][11:]+'.json'  # Path corresponding to 2nd article\n",
        "    if os.path.exists(h)==False:    # If the path does not exist the text data will be [-1,-1]\n",
        "        x.append([-1,-1])\n",
        "        continue\n",
        "    with open(h,'r') as f:    # If the path exists we read the data\n",
        "        f = json.load(f)\n",
        "    o.append(f['text'])\n",
        "    x.append(o)\n",
        "df = pd.DataFrame(x)\n",
        "df.to_csv('textdata.csv')  # Saving the text data as a separate csv file\n"
      ],
      "metadata": {
        "id": "OKNyA8x6Qjhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding labels to our text data\n",
        "df1 = pd.read_csv('input.csv')\n",
        "df2 = pd.read_csv('textdata.csv')\n",
        "df2['Geography'] = df1['Geography']\n",
        "df2['Entities'] = df1['Entities']\n",
        "df2['Time'] = df1['Time']\n",
        "df2['Narrative'] = df1['Narrative']\n",
        "df2['Overall'] = df1['Overall']\n",
        "df2['Style'] = df1['Style']\n",
        "df2['Tone'] = df1['Tone']\n",
        "z = []  # It will store the indices of each valid sample\n",
        "for i in range(df2.shape[0]):\n",
        "    if df2.iloc[i,1]=='-1' or df2.iloc[i,2]=='-1': # If it contains -1 it means its an invalid sample.\n",
        "        continue\n",
        "    else:\n",
        "        z.append(i)\n",
        "df2 = df2.iloc[z,1:]\n",
        "df2.to_csv('realdata.csv') # This csv contains our final data with valid samples"
      ],
      "metadata": {
        "id": "SM_dDQX4Uz-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for Model Implementation"
      ],
      "metadata": {
        "id": "mcJ2ey7B_QR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers # Will install necessary modules required for our analysis"
      ],
      "metadata": {
        "id": "UjIN6VgpdGLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-multilingual-cased') # We are using this particular BERT model from hugging face"
      ],
      "metadata": {
        "id": "5pw_Hi15dGI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('realdata.csv')\n",
        "\n",
        "z1 = model.encode(df[:,1])  # We are using this to encode the 1st news article in all the samples\n",
        "print(z1.shape) # Each sample is encoded into a 768 feature vector\n",
        "ok = pd.DataFrame(z1)\n",
        "ok.to_csv('hi1.csv')  # The encoded feature vector for the 1st article of each sample is stored in this csv file\n",
        "\n",
        "z2 = model.encode(df[:,2])  # We are using this to encode the 2st news article in all the samples\n",
        "print(z2.shape) # Each sample is encoded into a 768 feature vector\n",
        "ok = pd.DataFrame(z2)\n",
        "ok.to_csv('hi2.csv')  # The encoded feature vector for the 2nd article of each sample is stored in this csv file"
      ],
      "metadata": {
        "id": "-sro0aVAdTPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('realdata.csv')\n",
        "def length(a):  # This vector computes the magnitude of a vector\n",
        "    a = a*a\n",
        "    return np.sqrt(np.sum(a))\n",
        "def sigmoid(x):  # This implements the sigmoid function\n",
        "    return 1/(1+np.exp(-x))\n",
        "def bound(a):  # This function makes sure that all the predicted values are between [0,4]\n",
        "    for i in range(a.shape[0]):\n",
        "        if a[i]>4:\n",
        "            a[i] = 4\n",
        "        else:\n",
        "            if a[i]<0:\n",
        "                a[i] = 0\n",
        "    return a\n",
        "\n",
        "model = DecisionTreeRegressor()\n",
        "a1 = pd.read_csv('hi1.csv').values  # Taking the feature vectors of 1st article\n",
        "a2 = pd.read_csv('hi2.csv').values  # Taking the feature vectors of 2nd article\n",
        "a = a1*a2     # Converting 2 features into a single feature.\n",
        "y_label = (df[\"Overall\"].values)\n",
        "# Splitting the data into training and test dataset\n",
        "train_x,test_x,train_y,test_y = train_test_split(a,y_label,test_size = 0.3,shuffle = True,random_state = 42)\n",
        "model.fit(train_x,train_y) # Training the model and predicting the model \n",
        "train_pred = bound(model.predict(train_x)) # Making predictions for training and test datasets\n",
        "test_pred = bound(model.predict(test_x))\n",
        "print(\"Pearson Correlation coefficient on Training Data = \",pearsonr(train_pred,train_y)[0]) # Pearson coefficient for training data\n",
        "print(\"Pearson Correlation coefficient on Test Data = \",pearsonr(test_pred,test_y)[0]) # Pearson coefficient for test data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXGbiBIaeOK1",
        "outputId": "c57e6613-db72-4371-cf80-b41d26894635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pearson Correlation coefficient on Training Data =  0.9835387632076156\n",
            "Pearson Correlation coefficient on Test Data =  0.604725053572058\n"
          ]
        }
      ]
    }
  ]
}